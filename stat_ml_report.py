# -*- coding: utf-8 -*-
"""stat_ml_report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hHwlPJ6yWkPOJcC_TJQS5sU4djxgcIoW

ライブラリのインポート
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime
import time
import seaborn as sns
import zipfile
import re
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
import sklearn.metrics
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score

"""データの読み込み"""

df=pd.read_csv("googleplaystore.csv")
df.head(10)

"""前処理"""

#NaNを削除
print(f'NaNの数: {df.isnull().sum().sum()}')
df['Rating'] = df['Rating'].fillna(df['Rating'].median())
df.dropna(inplace=True)
print(f'NaNの数: {df.isnull().sum().sum()}')
print(f'データの数: {len(df)}')
#カテゴリカルデータ
df=pd.get_dummies(df,columns=['Category','Genres','Content Rating','Type'])
#日付を扱うためにに変換する
df['Last Updated']=pd.to_datetime(df['Last Updated'])
df['Last Updated']=df['Last Updated'].map(datetime.datetime.toordinal)
#扱わないデータを消去
df=df.drop(["App",'Current Ver','Android Ver'],axis=1)
#データを整形
df['Price']=[float(p[1:]) if '$' in p else float(p) for p in df['Price']]
df['Installs']=[int(ins.strip('+').replace(',','')) for ins in df['Installs']]
df['Reviews'] = df['Reviews'].astype(int)

#この処理はhttps://www.kaggle.com/data13/machine-learning-model-to-predict-app-rating-94　を参考にした
k_indices = df['Size'].loc[df['Size'].str.contains('k')].index.tolist()
converter = pd.DataFrame(df.loc[k_indices, 'Size'].apply(lambda x: x.strip('k')).astype(float).apply(lambda x: x / 1024).apply(lambda x: round(x, 3)).astype(str))
df.loc[k_indices,'Size'] = converter
df['Size'] = df['Size'].apply(lambda x: x.strip('M'))
df[df['Size'] == 'Varies with device'] = 0
df['Size'] = df['Size'].astype(float)

#Xとyに分け, さらにtrainとtestを分離する
y=df['Rating']
X=df.drop(['Rating'],axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)

"""主成分分析(PCA)"""

#PCA
from sklearn.decomposition import PCA
print(f'次元数:{len(df.columns)}')
n_components=10

pca=PCA(n_components=10)
pca.fit(X)
print(f'次元数(PCA):{n_components}')
X_pca=pca.transform(X)
X_train_pca,X_test_pca,y_train_pca,y_test_pca=train_test_split(X_pca,y,test_size=0.25,random_state=10)

"""モデルの評価関数"""

def model_valuation(y,y_pred):
  tmp={}
  tmp['RMSE']=round(sklearn.metrics.mean_squared_error(y,y_pred),4)
  tmp['MAE']=round(sklearn.metrics.mean_absolute_error(y,y_pred),4)
  tmp['R^2']=round(sklearn.metrics.r2_score(y,y_pred),4)

  return tmp

"""線形回帰"""

#PCAなし
lr=LinearRegression()
trainscore={}
testscore={}
linear_models=[lr,Ridge(),Lasso()]
for model in linear_models:
  model.fit(X_train,y_train)
  y_pred=model.predict(X_train)
  y_pred_test=model.predict(X_test)
  trainscore[model.__class__.__name__]=model_valuation(y_train,y_pred)
  testscore[model.__class__.__name__]=model_valuation(y_test,y_pred_test)
print("trainscore : ")
for i,j in trainscore.items():
  print(i,j)
print("testscore : ")
for i,j in testscore.items():
  print(i,j)

#PCAあり
lr=LinearRegression()
LR=LogisticRegression()
trainscore={}
testscore={}
linear_models=[lr,Ridge(),Lasso()]
for model in linear_models:
  model.fit(X_train_pca,y_train_pca)
  y_pred=model.predict(X_train_pca)
  y_pred_test=model.predict(X_test_pca)
  trainscore[model.__class__.__name__]=model_valuation(y_train_pca,y_pred)
  testscore[model.__class__.__name__]=model_valuation(y_test_pca,y_pred_test)
print("trainscore : ")
for i,j in trainscore.items():
  print(i,j)
print("testscore : ")
for i,j in testscore.items():
  print(i,j)

"""k-NN"""

#pcaなし

model = KNeighborsRegressor(n_neighbors=20)
model.fit(X_train, y_train)
train_pred=model.predict(X_train)
test_pred=model.predict(X_test)
print('train')
print(model_valuation(y_train,train_pred))
print('test')
print(model_valuation(y_test,test_pred))

#pcaあり
model = KNeighborsRegressor(n_neighbors=20)
model.fit(X_train_pca, y_train_pca)
train_pred=model.predict(X_train_pca)
test_pred=model.predict(X_test_pca)
print('train')
print(model_valuation(y_train_pca,train_pred))
print('test')
print(model_valuation(y_test_pca,test_pred))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
step=2 #レポートでは1
n_neighbors = np.arange(20, 50, step)
scores = []
scores_pca=[]
for n in n_neighbors:
    model.set_params(n_neighbors=n)
    model.fit(X_train, y_train)
    y_pred=model.predict(X_test)
    dic=model_valuation(y_test,y_pred)
    scores.append(dic['RMSE'])
for n in n_neighbors:
    model.set_params(n_neighbors=n)
    model.fit(X_train_pca, y_train_pca)
    y_pred_pca=model.predict(X_test_pca)
    dic=model_valuation(y_test_pca,y_pred_pca)
    scores_pca.append(dic['RMSE'])

fig,ax=plt.subplots()

ax.set_xlabel("Number of Neighbors K")
ax.set_ylabel("Score(RMSE)")
ax.plot(n_neighbors, scores,label='166 dim')
ax.grid(True)
ax.plot(n_neighbors,scores_pca,label='10 dim')
ax.legend()
plt.show()

Kmodel=KNeighborsRegressor(n_neighbors=38)
Kmodel.fit(X_train_pca,y_train_pca)
pred=Kmodel.predict(X_test_pca)
for i in range(600):
  if 0<list(y_test)[i]<2:
    print(f'予測値:{pred[i]},実際の値:{list(y_test)[i]}')

"""ランダムフォレスト+グリッドサーチ"""

#時間かかります
from sklearn.model_selection import RandomizedSearchCV

paramG = {'n_estimators':[197],'max_depth':[29,30,31]}
RFC_grid = GridSearchCV(estimator=RandomForestRegressor(random_state=0), param_grid=paramG,
                        scoring='r2', cv=3)
RFC_grid.fit(X_train,y_train)

print('ランダムサーチ・ランダムフォレストモデルにおける n_estimators  :  %d'  %RFC_grid.best_estimator_.n_estimators)

print('ランダムサーチ・ランダムフォレストモデルにおける max_depth  :  %d'  %RFC_grid.best_estimator_.max_depth)
print('グリッドサーチ・ランダムフォレストモデルによる予測値   :  %.3f'  %r2_score(y_test, RFC_grid.predict(X_test)))
pred_rf1=RFC_grid.predict(X_test)
print(model_valuation(y_test,pred_rf1))

#時間かかります

paramG = {'n_estimators':[197],'max_depth':[28,29,30,31,32]}
RFC_grid = GridSearchCV(estimator=RandomForestRegressor(random_state=0), param_grid=paramG,
                        scoring='r2', cv=3)
RFC_grid.fit(X_train_pca,y_train_pca)

print('ランダムサーチ・ランダムフォレストモデルにおける n_estimators  :  %d'  %RFC_grid.best_estimator_.n_estimators)

print('ランダムサーチ・ランダムフォレストモデルにおける max_depth  :  %d'  %RFC_grid.best_estimator_.max_depth)
print('グリッドサーチ・ランダムフォレストモデルによる予測値   :  %.3f'  %r2_score(y_test_pca, RFC_grid.predict(X_test_pca)))
pred=RFC_grid.predict(X_test_pca)
print(model_valuation(y_test_pca,pred))
for i in range(600):
  if 0<list(y_test_pca)[i]<2:
    print(f'予測値:{pred[i]},実際の値:{list(y_test_pca)[i]}')

max_plt=20
model=RandomForestRegressor(n_estimators=197,max_depth=31)
model.fit(X_train,y_train)
d=np.arange(max_plt)
FeatureDict={}
for i in range(len(X_train.columns)):
  FeatureDict[X_train.columns[i]]=model.feature_importances_[i]
FeatureDict = dict(sorted(FeatureDict.items(), key=lambda x:x[1],reverse=True))
print(FeatureDict)
y=list(FeatureDict.values())
z=list(FeatureDict.keys())
plt.yticks(d,z[:max_plt])
plt.barh(d,y[:max_plt])

"""lightGBM+グリッドサーチ"""

#pcaなし
import lightgbm
paramG={'max_depth':[14,16,18,20,22,24],'n_estimators':[40,45,50,55,60]}
gbm=lightgbm.LGBMRegressor()
RFC_grid=GridSearchCV(estimator=gbm,param_grid=paramG, scoring='r2',cv=3)
RFC_grid.fit(X_train,y_train)
ypred2=model.predict(X_test)

print('グリッドサーチ·lightGBMにおける n_estimators  :  %d'  %RFC_grid.best_estimator_.n_estimators)

print('グリッドサーチ·lightGBMにおける max_depth  :  %d'  %RFC_grid.best_estimator_.max_depth)
print('グリッドサーチ・lightGBMによる予測値   :  %.3f'  %r2_score(y_test, RFC_grid.predict(X_test)))
pred_gbm=RFC_grid.predict(X_test)
print(model_valuation(y_test,pred_gbm))
for i in range(600):
  if 0<list(y_test)[i]<2:
    print(f'予測値:{pred_gbm[i]},実際の値:{list(y_test)[i]}')

max_plt=20
model=lightgbm.LGBMRegressor(n_estimators=50,max_depth=14)
model.fit(X_train,y_train)
d=np.arange(max_plt)
FeatureDict={}
for i in range(len(X_train.columns)):
  FeatureDict[X_train.columns[i]]=model.feature_importances_[i]
FeatureDict = dict(sorted(FeatureDict.items(), key=lambda x:x[1],reverse=True))
print(FeatureDict)
y=list(FeatureDict.values())
z=list(FeatureDict.keys())
plt.yticks(d,z[:max_plt])
plt.barh(d,y[:max_plt])

#pcaあり
paramG={'max_depth':[14,16,18,20,22,24],'n_estimators':[40,45,50,55,60]}
gbm=lightgbm.LGBMRegressor()
RFC_grid=GridSearchCV(estimator=gbm,param_grid=paramG, scoring='r2',cv=3)
RFC_grid.fit(X_train_pca,y_train_pca)
ypred2=model.predict(X_test)

print('グリッドサーチ·lightGBMにおける n_estimators  :  %d'  %RFC_grid.best_estimator_.n_estimators)

print('グリッドサーチ·lightGBMにおける max_depth  :  %d'  %RFC_grid.best_estimator_.max_depth)
print('グリッドサーチ・lightGBMによる予測値   :  %.3f'  %r2_score(y_test, RFC_grid.predict(X_test_pca)))
pred=RFC_grid.predict(X_test_pca)
print(model_valuation(y_test_pca,pred))
for i in range(600):
  if 0<list(y_test)[i]<2:
    print(f'予測値:{pred[i]},実際の値:{list(y_test_pca)[i]}')

#ランダムフォレストとlightgbmの比較
for i in range(len(y_test_pca)):
  if 4.5<list(y_test_pca)[i]<5 and pred[i]<4:
    print(f'実際の値:{list(y_test_pca)[i]}')
    print(f'ランダムフォレスト:{pred[i]}')
    print(f'    \tlightGBM:{pred_gbm[i]}')
    print('\n')

